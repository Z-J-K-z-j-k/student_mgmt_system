# server/crawler.py
import random
import time
import requests
from bs4 import BeautifulSoup
from .models import get_conn

# ============================================================
# å­¦ç”Ÿæ•°æ®çˆ¬å–ï¼ˆç¤ºä¾‹ï¼‰
# ============================================================

def crawl_dummy_students():
    """
    ç¤ºä¾‹ï¼šä»ä¸€ä¸ªå…¬å¼€ç½‘é¡µæŠ“ä¸€äº›åå­—ï¼ˆä½ å¯ä»¥æ¢æˆå­¦æ ¡æ–°é—»ç­‰ï¼‰ï¼Œ
    ç„¶åç”Ÿæˆå‡å­¦ç”Ÿå†™å…¥æ•°æ®åº“ã€‚
    """
    url = "https://www.renmingmingzi.com/100gehaotingdexingming.html"  # åªæ˜¯ä¸ªç¤ºä¾‹ç½‘ç«™
    try:
        resp = requests.get(url, timeout=5)
        resp.encoding = resp.apparent_encoding
    except Exception as e:
        print("âš  çˆ¬å–å¤±è´¥ï¼š", e)
        return 0

    soup = BeautifulSoup(resp.text, "html.parser")
    names = [tag.get_text(strip=True) for tag in soup.find_all("p")][:30]
    majors = ["è®¡ç®—æœº", "äººå·¥æ™ºèƒ½", "é€šä¿¡å·¥ç¨‹", "è½¯ä»¶å·¥ç¨‹"]
    classes = ["1ç­", "2ç­", "3ç­"]

    added = 0
    with get_conn() as conn:
        cur = conn.cursor()
        for i, name in enumerate(names):
            sno = f"2024{200+i:03d}"
            gender = random.choice(["ç”·", "å¥³"])
            major = random.choice(majors)
            class_name = major + random.choice(classes)
            try:
                # æ³¨æ„ï¼šcreate_table.sql ä¸­ students è¡¨æ²¡æœ‰ student_no å­—æ®µ
                # ä½¿ç”¨ name, gender, major, class_name ç­‰å­—æ®µ
                cur.execute("""
                INSERT INTO students (name, gender, major, class_name)
                VALUES (%s, %s, %s, %s)
                """, (name, gender, major, class_name))
                added += 1
            except Exception:
                conn.rollback()
    print(f"âœ… çˆ¬è™«å¯¼å…¥å­¦ç”Ÿ {added} æ¡")
    return added

# ============================================================
# åŒ—é‚®è®¡ç®—æœºå­¦é™¢æ•™å¸ˆçˆ¬å–
# ============================================================

BUPT_SCS_URL = "https://scs.bupt.edu.cn/szjs1/jsyl.htm"


def fetch_page(url, timeout=10):
    """
    è·å–ç½‘é¡µå†…å®¹
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.encoding = "utf-8"
        return resp.text
    except Exception as e:
        print(f"âš  è·å–ç½‘é¡µå¤±è´¥ï¼š{e}")
        raise


def parse_teacher_links(html):
    """
    ä»ç´¢å¼•é¡µé¢è§£ææ•™å¸ˆé“¾æ¥
    è¿”å›æ•™å¸ˆé“¾æ¥åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¥åŒ…å« (å§“å, URL)
    """
    import re
    
    try:
        soup = BeautifulSoup(html, "lxml")
    except:
        soup = BeautifulSoup(html, "html.parser")
    
    teacher_links = []
    
    # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰ teacher_table è¡¨æ ¼ä¸­çš„é“¾æ¥
    teacher_tables = soup.find_all("table", class_="teacher_table")
    
    for table in teacher_tables:
        # æŸ¥æ‰¾è¡¨æ ¼ä¸­æ‰€æœ‰çš„é“¾æ¥
        links = table.find_all("a", href=True)
        for link in links:
            href = link.get("href", "")
            text = link.get_text(strip=True)
            title = link.get("title", "")
            
            # ä½¿ç”¨titleå±æ€§æˆ–æ–‡æœ¬ä½œä¸ºå§“å
            name = title if title else text
            
            # è·³è¿‡ç©ºé“¾æ¥
            if not name or len(name) < 2:
                continue
            
            # è·³è¿‡æ˜æ˜¾ä¸æ˜¯æ•™å¸ˆå§“åçš„
            if name in ["æ›´å¤š", "æŸ¥çœ‹", "è¯¦æƒ…", "è¿”å›", "é¦–é¡µ", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "ã€€"]:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­æ–‡å§“åï¼ˆ2-4ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
            name_pattern = re.compile(r'^[\u4e00-\u9fa5]{2,4}$')
            if name_pattern.match(name):
                # æ„å»ºå®Œæ•´URL
                if href.startswith("http"):
                    full_url = href
                elif href.startswith("/"):
                    full_url = "https://scs.bupt.edu.cn" + href
                elif href.startswith("../"):
                    # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ ¹æ®å½“å‰é¡µé¢URLæ„å»º
                    full_url = "https://scs.bupt.edu.cn/" + href.replace("../", "")
                elif href.startswith("#"):
                    # è·³è¿‡é”šç‚¹é“¾æ¥
                    continue
                else:
                    # å…¶ä»–ç›¸å¯¹è·¯å¾„
                    base_url = BUPT_SCS_URL.rsplit("/", 1)[0] + "/"
                    full_url = base_url + href
                
                teacher_links.append((name, full_url))
    
    # å¦‚æœè¡¨æ ¼æ–¹æ³•æ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šç”¨æ–¹æ³•
    if not teacher_links:
        print("âš  æœªåœ¨teacher_tableä¸­æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
        all_links = soup.find_all("a", href=True)
        
        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True)
            title = link.get("title", "")
            
            name = title if title else text
            
            if not name or len(name) < 2:
                continue
            if name in ["æ›´å¤š", "æŸ¥çœ‹", "è¯¦æƒ…", "è¿”å›", "é¦–é¡µ", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ"]:
                continue
            if href.startswith("#") or href.startswith("javascript:"):
                continue
            
            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯ä¸­æ–‡å§“å
            name_pattern = re.compile(r'^[\u4e00-\u9fa5]{2,4}$')
            if name_pattern.match(name):
                if href.startswith("http"):
                    full_url = href
                elif href.startswith("/"):
                    full_url = "https://scs.bupt.edu.cn" + href
                else:
                    base_url = BUPT_SCS_URL.rsplit("/", 1)[0] + "/"
                    full_url = base_url + href
                
                teacher_links.append((name, full_url))
    
    # å»é‡ï¼ˆæŒ‰å§“åï¼‰
    seen_names = set()
    unique_links = []
    for name, url in teacher_links:
        if name not in seen_names:
            seen_names.add(name)
            unique_links.append((name, url))
    
    print(f"âœ… ä»ç´¢å¼•é¡µé¢æ‰¾åˆ° {len(unique_links)} ä¸ªæ•™å¸ˆé“¾æ¥")
    return unique_links


def parse_teacher_detail(html, default_name=""):
    """
    ä»æ•™å¸ˆè¯¦æƒ…é¡µé¢è§£ææ•™å¸ˆä¿¡æ¯
    """
    import re
    
    try:
        soup = BeautifulSoup(html, "lxml")
    except:
        soup = BeautifulSoup(html, "html.parser")
    
    teacher = {
        "name": default_name,
        "title": "",
        "department": "",
        "research": "",
        "email": "",
        "homepage": ""
    }
    
    # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬
    page_text = soup.get_text()
    
    # æå–å§“åï¼ˆå¦‚æœé¡µé¢ä¸­æœ‰ï¼‰
    if not teacher["name"]:
        name_pattern = re.compile(r'[\u4e00-\u9fa5]{2,4}')
        name_match = name_pattern.search(page_text[:500])  # åœ¨å‰500å­—ç¬¦ä¸­æŸ¥æ‰¾
        if name_match:
            teacher["name"] = name_match.group(0)
    
    # æå–èŒç§°
    title_keywords = ["æ•™æˆ", "å‰¯æ•™æˆ", "è®²å¸ˆ", "åŠ©ç†æ•™æˆ", "ç ”ç©¶å‘˜", "å‰¯ç ”ç©¶å‘˜", "é«˜çº§å·¥ç¨‹å¸ˆ"]
    for keyword in title_keywords:
        if keyword in page_text:
            teacher["title"] = keyword
            break
    
    # æå–ç³»åˆ«/éƒ¨é—¨
    dept_keywords = ["ç³»", "å­¦é™¢", "ç ”ç©¶æ‰€", "ä¸­å¿ƒ"]
    for keyword in dept_keywords:
        dept_match = re.search(r'[\u4e00-\u9fa5]+' + keyword, page_text)
        if dept_match:
            teacher["department"] = dept_match.group(0)
            break
    
    # æå–é‚®ç®±
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    email_match = email_pattern.search(page_text)
    if email_match:
        teacher["email"] = email_match.group(0)
    
    # å°è¯•é€šè¿‡é€‰æ‹©å™¨æŸ¥æ‰¾ç»“æ„åŒ–ä¿¡æ¯
    # å¸¸è§çš„æ ‡ç­¾æ¨¡å¼
    info_selectors = {
        "name": ["h1", "h2", ".name", "[class*='name']"],
        "title": [".title", "[class*='title']", ".zc", "[class*='zc']"],
        "department": [".dept", "[class*='dept']", ".xy", "[class*='xy']"],
        "research": [".research", "[class*='research']", ".fx", "[class*='fx']"],
        "email": [".email", "[class*='email']", "a[href^='mailto:']"]
    }
    
    for key, selectors in info_selectors.items():
        if teacher[key]:  # å¦‚æœå·²ç»æ‰¾åˆ°ï¼Œè·³è¿‡
            continue
        for selector in selectors:
            tag = soup.select_one(selector)
            if tag:
                if key == "email" and tag.name == "a":
                    href = tag.get("href", "")
                    if href.startswith("mailto:"):
                        teacher["email"] = href.replace("mailto:", "").strip()
                else:
                    text = tag.get_text(strip=True)
                    if text and len(text) < 100:  # é¿å…è·å–å¤§æ®µæ–‡æœ¬
                        teacher[key] = text
                break
    
    return teacher


def parse_teachers(html):
    """
    è§£ææ•™å¸ˆä¿¡æ¯ï¼ˆå…¼å®¹ç´¢å¼•é¡µå’Œè¯¦æƒ…é¡µï¼‰
    å¦‚æœæ˜¯ç´¢å¼•é¡µï¼Œè¿”å›é“¾æ¥åˆ—è¡¨ï¼›å¦‚æœæ˜¯è¯¦æƒ…é¡µï¼Œè¿”å›æ•™å¸ˆä¿¡æ¯
    """
    # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¸»è¦ç”¨äºç´¢å¼•é¡µï¼Œè¿”å›é“¾æ¥
    return parse_teacher_links(html)


def clean_teacher_data(teacher):
    """
    æ•°æ®æ¸…æ´—ï¼šæ¸…ç†å’Œè§„èŒƒåŒ–æ•™å¸ˆæ•°æ®
    """
    # æ¸…ç†å§“åï¼šå»é™¤å¤šä½™ç©ºæ ¼
    teacher["name"] = " ".join(teacher["name"].split())
    
    # è§„èŒƒåŒ–èŒç§°
    title = teacher["title"]
    if title:
        # ç»Ÿä¸€èŒç§°æ ¼å¼
        title_mapping = {
            "æ•™æˆ": "æ•™æˆ",
            "å‰¯æ•™æˆ": "å‰¯æ•™æˆ",
            "è®²å¸ˆ": "è®²å¸ˆ",
            "åŠ©ç†æ•™æˆ": "åŠ©ç†æ•™æˆ",
            "ç ”ç©¶å‘˜": "ç ”ç©¶å‘˜",
            "å‰¯ç ”ç©¶å‘˜": "å‰¯ç ”ç©¶å‘˜",
        }
        for key, value in title_mapping.items():
            if key in title:
                teacher["title"] = value
                break
    
    # æ¸…ç†é‚®ç®±ï¼šéªŒè¯æ ¼å¼
    email = teacher["email"]
    if email and "@" not in email:
        teacher["email"] = ""
    
    # æ¸…ç†ç³»åˆ«ï¼šç»Ÿä¸€æ ¼å¼
    dept = teacher["department"]
    if dept:
        # å»é™¤"ç³»"å­—åçš„å¤šä½™å†…å®¹ï¼Œç»Ÿä¸€ä¸º"XXç³»"
        if "ç³»" in dept:
            dept = dept.split("ç³»")[0] + "ç³»"
        teacher["department"] = dept
    
    return teacher


def crawl_bupt_scs_teachers(max_teachers=None, delay=1):
    """
    çˆ¬å–åŒ—äº¬é‚®ç”µå¤§å­¦è®¡ç®—æœºå­¦é™¢æ•™å¸ˆåå½•å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
    å‚æ•°:
        max_teachers: æœ€å¤§çˆ¬å–æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        delay: è®¿é—®æ¯ä¸ªæ•™å¸ˆä¸»é¡µçš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
    è¿”å›ï¼š(æˆåŠŸæ•°é‡, è·³è¿‡æ•°é‡, é”™è¯¯ä¿¡æ¯)
    """
    print("å¼€å§‹çˆ¬å–åŒ—äº¬é‚®ç”µå¤§å­¦è®¡ç®—æœºå­¦é™¢æ•™å¸ˆåå½•â€¦")
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–ç´¢å¼•é¡µé¢ï¼Œæå–æ•™å¸ˆé“¾æ¥
        print("ğŸ“‹ æ­¥éª¤1: è·å–æ•™å¸ˆåˆ—è¡¨ç´¢å¼•é¡µ...")
        html = fetch_page(BUPT_SCS_URL)
        teacher_links = parse_teacher_links(html)
        
        if not teacher_links:
            return 0, 0, "æœªæ‰¾åˆ°æ•™å¸ˆé“¾æ¥ï¼Œè¯·æ£€æŸ¥ç½‘é¡µç»“æ„æ˜¯å¦å˜åŒ–"
        
        print(f"âœ… æ‰¾åˆ° {len(teacher_links)} ä¸ªæ•™å¸ˆé“¾æ¥")
        
        # é™åˆ¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_teachers:
            teacher_links = teacher_links[:max_teachers]
            print(f"âš  é™åˆ¶çˆ¬å–æ•°é‡ä¸º {max_teachers}")
        
        # ç¬¬äºŒæ­¥ï¼šè®¿é—®æ¯ä¸ªæ•™å¸ˆä¸»é¡µï¼Œè·å–è¯¦ç»†ä¿¡æ¯
        print(f"ğŸ“‹ æ­¥éª¤2: å¼€å§‹è®¿é—®æ•™å¸ˆä¸»é¡µï¼ˆå…± {len(teacher_links)} ä¸ªï¼‰...")
        
        added = 0
        skipped = 0
        errors = []
        
        with get_conn() as conn:
            cur = conn.cursor()
            
            for idx, (name, url) in enumerate(teacher_links, 1):
                try:
                    print(f"  [{idx}/{len(teacher_links)}] æ­£åœ¨å¤„ç†: {name}...", end=" ")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    cur.execute("SELECT teacher_id FROM teachers WHERE name = %s", (name,))
                    if cur.fetchone():
                        print("å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        skipped += 1
                        continue
                    
                    # è®¿é—®æ•™å¸ˆä¸»é¡µ
                    try:
                        detail_html = fetch_page(url)
                        teacher = parse_teacher_detail(detail_html, default_name=name)
                    except Exception as e:
                        print(f"è®¿é—®å¤±è´¥: {e}")
                        # å³ä½¿è®¿é—®å¤±è´¥ï¼Œä¹Ÿå°è¯•ç”¨ç´¢å¼•é¡µçš„å§“ååˆ›å»ºåŸºæœ¬è®°å½•
                        teacher = {
                            "name": name,
                            "title": "",
                            "department": "è®¡ç®—æœºå­¦é™¢",
                            "research": "",
                            "email": "",
                            "homepage": url
                        }
                    
                    # æ•°æ®æ¸…æ´—
                    teacher = clean_teacher_data(teacher)
                    
                    if not teacher["name"]:
                        print("å§“åæ— æ•ˆï¼Œè·³è¿‡")
                        skipped += 1
                        continue
                    
                    # æ’å…¥æ•°æ®åº“
                    # æ³¨æ„ï¼šcreate_table.sql ä¸­ teachers è¡¨å­—æ®µä¸º department è€Œä¸æ˜¯ dept
                    cur.execute("""
                        INSERT INTO teachers (name, department, title, email, research)
                        VALUES (%s, %s, %s, %s, %s)
                    """, (
                        teacher["name"],
                        teacher["department"] or "è®¡ç®—æœºå­¦é™¢",
                        teacher["title"],
                        teacher["email"],
                        teacher.get("research", "")
                    ))
                    print("âœ… æˆåŠŸ")
                    added += 1
                    
                    # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    if delay > 0 and idx < len(teacher_links):
                        time.sleep(delay)
                    
                except Exception as e:
                    error_msg = f"å¤„ç†æ•™å¸ˆ {name} æ—¶å‡ºé”™ï¼š{str(e)}"
                    errors.append(error_msg)
                    print(f"âŒ å¤±è´¥: {e}")
                    continue
        
        result_msg = f"âœ… å®Œæˆï¼æˆåŠŸå¯¼å…¥ {added} åæ•™å¸ˆï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®"
        if errors:
            result_msg += f"ï¼Œ{len(errors)} æ¡é”™è¯¯"
        print(result_msg)
        
        return added, skipped, "; ".join(errors) if errors else None
        
    except Exception as e:
        error_msg = f"çˆ¬å–å¤±è´¥ï¼š{str(e)}"
        print(f"âŒ {error_msg}")
        import traceback
        traceback.print_exc()
        return 0, 0, error_msg


def debug_page_structure(url=BUPT_SCS_URL):
    """
    è°ƒè¯•å‡½æ•°ï¼šè·å–å¹¶ä¿å­˜ç½‘é¡µå†…å®¹ï¼Œåˆ†æç»“æ„
    """
    print("=" * 60)
    print("è°ƒè¯•æ¨¡å¼ï¼šåˆ†æç½‘é¡µç»“æ„")
    print("=" * 60)
    
    try:
        html = fetch_page(url)
        
        # ä¿å­˜åŸå§‹HTMLåˆ°æ–‡ä»¶
        with open("bupt_page_debug.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("âœ… ç½‘é¡µå†…å®¹å·²ä¿å­˜åˆ° bupt_page_debug.html")
        
        # è§£æå¹¶åˆ†æ
        soup = BeautifulSoup(html, "html.parser")
        
        print(f"\né¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— '}")
        print(f"é¡µé¢æ€»å­—ç¬¦æ•°: {len(html)}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å®¹å™¨
        print("\næŸ¥æ‰¾å¯èƒ½çš„æ•™å¸ˆå®¹å™¨...")
        containers = [
            ("div.teacher_li", soup.select("div.teacher_li")),
            ("div[class*='teacher']", soup.select("div[class*='teacher']")),
            ("li[class*='teacher']", soup.select("li[class*='teacher']")),
            ("table tr", soup.select("table tr")),
            ("div.list-item", soup.select("div.list-item")),
            ("ul li", soup.select("ul li")[:20]),  # é™åˆ¶æ•°é‡
        ]
        
        for selector, items in containers:
            if items:
                print(f"  âœ… {selector}: æ‰¾åˆ° {len(items)} ä¸ª")
                # æ‰“å°ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»“æ„
                if len(items) > 0:
                    first = items[0]
                    print(f"     ç¬¬ä¸€ä¸ªå…ƒç´ : {first.name}, class={first.get('class')}")
                    print(f"     æ–‡æœ¬é¢„è§ˆ: {first.get_text(strip=True)[:100]}")
            else:
                print(f"  âŒ {selector}: æœªæ‰¾åˆ°")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä¸­æ–‡å§“åçš„å…ƒç´ 
        import re
        name_pattern = re.compile(r'[\u4e00-\u9fa5]{2,4}')
        potential_names = []
        for tag in soup.find_all(["div", "li", "td", "span", "p"]):
            text = tag.get_text(strip=True)
            if name_pattern.match(text) and 2 <= len(text) <= 4:
                if text not in ["å§“å", "èŒç§°", "ç³»åˆ«", "é‚®ç®±", "ç ”ç©¶æ–¹å‘", "æ›´å¤š", "æŸ¥çœ‹"]:
                    potential_names.append((tag.name, tag.get("class"), text))
        
        if potential_names:
            print(f"\næ‰¾åˆ° {len(potential_names)} ä¸ªå¯èƒ½çš„å§“å:")
            for tag_name, classes, name in potential_names[:20]:
                print(f"  {tag_name} (class={classes}): {name}")
        
        return html
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "debug":
        # è°ƒè¯•æ¨¡å¼
        debug_page_structure()
    else:
        # æ­£å¸¸çˆ¬è™«æ¨¡å¼
        added, skipped, error = crawl_bupt_scs_teachers()
        print(f"ç»“æœï¼šæˆåŠŸ {added}ï¼Œè·³è¿‡ {skipped}ï¼Œé”™è¯¯ï¼š{error or 'æ— '}")
